# Docker Layer CVE Mapping Implementation

## 1. Extract Packages from Layer

### Debian/Ubuntu Layer Analysis
```python
def extract_packages_from_layer(layer_filesystem):
    """Extract exact package versions from Debian/Ubuntu layer."""
    
    # Parse /var/lib/dpkg/status
    dpkg_status = layer_filesystem.read("/var/lib/dpkg/status")
    packages = []
    
    current_package = {}
    for line in dpkg_status.split('\n'):
        if line.startswith('Package: '):
            if current_package and current_package.get('Status') == 'install ok installed':
                packages.append(Package(
                    name=current_package['Package'],
                    version=current_package['Version'],
                    architecture=current_package.get('Architecture', 'amd64'),
                    distro='debian'  # or 'ubuntu'
                ))
            current_package = {'Package': line.split(': ', 1)[1]}
        
        elif line.startswith('Version: '):
            current_package['Version'] = line.split(': ', 1)[1]
        
        elif line.startswith('Status: '):
            current_package['Status'] = line.split(': ', 1)[1]
        
        elif line.startswith('Architecture: '):
            current_package['Architecture'] = line.split(': ', 1)[1]
    
    return packages

# Example output:
# Package(name='nginx', version='1.18.0-0ubuntu1.2', architecture='amd64', distro='ubuntu')
```

### Alpine Layer Analysis
```python
def extract_alpine_packages(layer_filesystem):
    """Extract packages from Alpine layer."""
    
    # Parse /lib/apk/db/installed
    apk_db = layer_filesystem.read("/lib/apk/db/installed")
    packages = []
    
    current_package = {}
    for line in apk_db.split('\n'):
        if line.startswith('P:'):  # Package name
            current_package['name'] = line[2:]
        elif line.startswith('V:'):  # Version
            current_package['version'] = line[2:]
        elif line.startswith('A:'):  # Architecture
            current_package['arch'] = line[2:]
        elif line == '':  # End of package entry
            if current_package:
                packages.append(Package(
                    name=current_package['name'],
                    version=current_package['version'],
                    architecture=current_package.get('arch', 'x86_64'),
                    distro='alpine'
                ))
                current_package = {}
    
    return packages
```

## 2. Map Packages to CVEs

### CVE Database Integration
```python
class CVEMapper:
    def __init__(self):
        # Initialize vulnerability databases
        self.ubuntu_db = UbuntuSecurityDB()
        self.debian_db = DebianSecurityDB()
        self.alpine_db = AlpineSecurityDB()
        self.nvd_db = NVDDB()
    
    def map_package_to_cves(self, package):
        """Map exact package version to CVEs."""
        
        vulnerabilities = []
        
        # 1. Query distro-specific database first (most accurate)
        if package.distro == 'ubuntu':
            vulns = self.ubuntu_db.query(package.name, package.version)
        elif package.distro == 'debian':
            vulns = self.debian_db.query(package.name, package.version)
        elif package.distro == 'alpine':
            vulns = self.alpine_db.query(package.name, package.version)
        
        # 2. Cross-reference with NVD for additional CVEs
        nvd_vulns = self.nvd_db.query_by_cpe(
            f"cpe:2.3:a:*:{package.name}:{package.version}:*:*:*:*:*:*:*"
        )
        
        # 3. Combine and deduplicate
        return self.merge_vulnerability_data(vulns, nvd_vulns)

class UbuntuSecurityDB:
    def __init__(self):
        # Use Ubuntu Security Notices (USN) database
        self.db_url = "https://ubuntu.com/security/notices"
        
    def query(self, package_name, version):
        """Query Ubuntu security database."""
        
        # Example: nginx 1.18.0-0ubuntu1.2 in Ubuntu 20.04
        # Check if this version is affected by any USNs
        
        usns = self.get_usns_for_package(package_name)
        vulnerabilities = []
        
        for usn in usns:
            if self.is_version_affected(version, usn.affected_versions):
                vulnerabilities.append({
                    'cve': usn.cve_ids,
                    'severity': usn.severity,
                    'fixed_version': usn.fixed_version,
                    'source': f'USN-{usn.id}',
                    'description': usn.description
                })
        
        return vulnerabilities
```

## 3. Map Back to Source Code Location

### Dockerfile Line Mapping
```python
class DockerSourceMapper:
    def __init__(self, dockerfile_path):
        self.dockerfile = self.parse_dockerfile(dockerfile_path)
        self.layer_to_instruction = {}  # layer_id -> instruction_line
    
    def map_vulnerability_to_source(self, vulnerability, layer_id):
        """Map vulnerability back to Dockerfile line."""
        
        # Find which instruction created this layer
        instruction = self.layer_to_instruction.get(layer_id)
        
        if not instruction:
            return None
        
        # Create source location
        return SourceLocation(
            file_path=self.dockerfile_path,
            line_number=instruction.line_number,
            declaration=instruction.original_text,
            context=f"Layer {layer_id[:12]} introduced vulnerable package {vulnerability.package}"
        )
    
    def build_layer_mapping(self, build_output):
        """Parse Docker build output to map layers to instructions."""
        
        current_instruction = None
        
        for line in build_output:
            # Match Docker build step
            step_match = re.match(r'Step (\d+)/\d+ : (.+)', line)
            if step_match:
                step_num = int(step_match.group(1))
                current_instruction = self.dockerfile.instructions[step_num - 1]
            
            # Match layer creation
            layer_match = re.match(r' ---> ([a-f0-9]+)', line)
            if layer_match and current_instruction:
                layer_id = layer_match.group(1)
                self.layer_to_instruction[layer_id] = current_instruction

# Example usage:
mapper = DockerSourceMapper("Dockerfile")
vulnerability = Vulnerability(package="nginx", cve="CVE-2021-23017")
source_location = mapper.map_vulnerability_to_source(vulnerability, layer_id)

# Result:
# SourceLocation(
#   file_path="Dockerfile",
#   line_number=15,
#   declaration="RUN apt-get install -y nginx",
#   context="Layer abc123 introduced vulnerable package nginx"
# )
```

## 4. Efficient Implementation for Laptops

### Memory-Efficient Layer Processing
```python
class EfficientDockerScanner:
    def __init__(self):
        self.max_memory_mb = 512  # Laptop-friendly limit
        self.cache_dir = Path.home() / ".sca_cache"
        
    def scan_image_efficiently(self, image_name):
        """Scan Docker image without exhausting laptop resources."""
        
        # 1. Stream layers one at a time (don't load all in memory)
        manifest = self.get_image_manifest(image_name)
        
        all_vulnerabilities = []
        processed_packages = set()  # Avoid duplicate analysis
        
        for layer in manifest.layers:
            # Check cache first
            cache_key = f"{layer.digest}.json"
            cached_result = self.load_from_cache(cache_key)
            
            if cached_result:
                all_vulnerabilities.extend(cached_result)
                continue
            
            # Process layer in chunks
            layer_vulns = self.process_layer_streaming(layer)
            
            # Cache results
            self.save_to_cache(cache_key, layer_vulns)
            all_vulnerabilities.extend(layer_vulns)
            
            # Memory cleanup
            gc.collect()
        
        return self.deduplicate_vulnerabilities(all_vulnerabilities)
    
    def process_layer_streaming(self, layer):
        """Process layer without loading entire filesystem in memory."""
        
        vulnerabilities = []
        
        # Download layer as stream
        with self.download_layer_stream(layer.digest) as stream:
            # Extract only package database files
            with tarfile.open(fileobj=stream, mode='r|*') as tar:
                for member in tar:
                    # Only extract package databases
                    if self.is_package_database(member.name):
                        # Extract to temporary file
                        with tempfile.NamedTemporaryFile() as temp:
                            shutil.copyfileobj(tar.extractfile(member), temp)
                            temp.flush()
                            
                            # Parse packages
                            packages = self.parse_package_db(temp.name, member.name)
                            
                            # Map to CVEs immediately (don't store packages)
                            for package in packages:
                                vulns = self.map_package_to_cves(package)
                                vulnerabilities.extend(vulns)
        
        return vulnerabilities
```

### Parallel Processing with Resource Limits
```python
import concurrent.futures
import psutil

class OptimizedScanner:
    def __init__(self):
        # Detect system resources
        self.cpu_count = min(psutil.cpu_count(), 4)  # Max 4 threads on laptop
        self.max_memory = psutil.virtual_memory().available * 0.3  # Use 30% of available RAM
        
    def scan_layers_parallel(self, layers):
        """Process multiple layers in parallel with resource limits."""
        
        vulnerabilities = []
        
        # Process in batches to control memory usage
        batch_size = min(len(layers), self.cpu_count)
        
        for i in range(0, len(layers), batch_size):
            batch = layers[i:i + batch_size]
            
            # Process batch in parallel
            with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:
                futures = [
                    executor.submit(self.process_single_layer, layer)
                    for layer in batch
                ]
                
                # Collect results
                for future in concurrent.futures.as_completed(futures):
                    try:
                        batch_vulns = future.result(timeout=60)  # 1 minute timeout
                        vulnerabilities.extend(batch_vulns)
                    except Exception as e:
                        logger.error(f"Layer processing failed: {e}")
                
            # Memory check between batches
            if psutil.virtual_memory().percent > 80:
                logger.warning("High memory usage, pausing...")
                time.sleep(2)
                gc.collect()
        
        return vulnerabilities
```

### Aggressive Caching Strategy
```python
class DockerScanCache:
    def __init__(self):
        self.cache_dir = Path.home() / ".sca_cache" / "docker"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Layer cache: digest -> vulnerabilities
        self.layer_cache = {}
        
        # Base image cache: image:tag -> packages
        self.base_image_cache = {}
    
    def get_layer_vulnerabilities(self, layer_digest):
        """Get cached vulnerabilities for layer."""
        cache_file = self.cache_dir / f"{layer_digest}.json"
        
        if cache_file.exists():
            # Check age (invalidate after 24 hours)
            if time.time() - cache_file.stat().st_mtime < 86400:
                with open(cache_file) as f:
                    return json.load(f)
        
        return None
    
    def cache_layer_vulnerabilities(self, layer_digest, vulnerabilities):
        """Cache vulnerabilities for layer."""
        cache_file = self.cache_dir / f"{layer_digest}.json"
        
        with open(cache_file, 'w') as f:
            json.dump(vulnerabilities, f, indent=2)
    
    def estimate_cache_hit_rate(self, image_name):
        """Estimate how much work can be skipped due to caching."""
        manifest = self.get_image_manifest(image_name)
        
        cached_layers = 0
        total_layers = len(manifest.layers)
        
        for layer in manifest.layers:
            if self.get_layer_vulnerabilities(layer.digest):
                cached_layers += 1
        
        hit_rate = cached_layers / total_layers if total_layers > 0 else 0
        
        logger.info(f"Cache hit rate: {hit_rate:.1%} ({cached_layers}/{total_layers} layers)")
        return hit_rate
```

## Performance Benchmarks (Laptop-Friendly)

### Target Performance Goals
```python
PERFORMANCE_TARGETS = {
    "small_image": {  # alpine:3.14 (~5MB)
        "scan_time": "< 10 seconds",
        "memory_usage": "< 100MB",
        "cache_hit": "> 80% on second scan"
    },
    "medium_image": {  # ubuntu:20.04 (~72MB)
        "scan_time": "< 30 seconds", 
        "memory_usage": "< 200MB",
        "cache_hit": "> 70% on second scan"
    },
    "large_image": {  # node:16 (~900MB)
        "scan_time": "< 2 minutes",
        "memory_usage": "< 400MB", 
        "cache_hit": "> 60% on second scan"
    }
}
```

## Summary: Practical Implementation

1. **CVE Detection**: Parse exact packages from layer filesystems, query distro-specific databases
2. **Source Mapping**: Track layer creation during build, map vulnerabilities back to Dockerfile lines  
3. **Laptop Efficiency**: Stream processing, aggressive caching, resource limits, parallel batching

The key insight is that most Docker images share common base layers, so caching provides massive performance improvements on repeated scans.